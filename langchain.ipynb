{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5aa79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e224e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model=ChatGroq(api_key=os.getenv(\"GROQ_API_KEY\") , model=\"llama-3.1-8b-instant\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74e893",
   "metadata": {},
   "source": [
    "#### - Simple Model invoking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2655768",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=model.invoke(\"What is Maths? . explain math in one sentence\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd23e0e",
   "metadata": {},
   "source": [
    "### -Streaming O/P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a732e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model.stream(\"generate 1000 word paragraph about Machine Learning\"):\n",
    "    print(chunk.content  , end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f3375",
   "metadata": {},
   "source": [
    "## Looping through list of questions... (batching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fdef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can send multiple prompts as input in one single request...\n",
    "requests= [\n",
    "    \"who is Quaid-e-Azam\",\n",
    "    \"who is Allama-Iqbal\",\n",
    "    \"who is Rock\"\n",
    "]\n",
    "\n",
    "print(\"Processing your input :) \")\n",
    "\n",
    "multiple_responses=model.batch(\n",
    "    requests,\n",
    "    config={\"max_concurrency\":1}\n",
    ")\n",
    "\n",
    "print(multiple_responses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342cbf9",
   "metadata": {},
   "source": [
    "### Using prompt tempelates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-importing chat prompt tempelate.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#-define a Prompt Template with vairable slot\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"Explain {topic} to a 5 year old kid.\"\n",
    ")\n",
    "gen_response=prompt.invoke({\"topic\":\"Civil Engineering\"})\n",
    "print(\"Human Message ----->\",gen_response) #----- topic civil engeering is automatically specifed to variable topic .\n",
    "\n",
    "gen2=model.invoke(gen_response)\n",
    "print(gen2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5488eaeb",
   "metadata": {},
   "source": [
    "#### Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7934b622",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063469b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "#-create parser object\n",
    "parser=StrOutputParser()\n",
    "# before invoking the parser we must have ai generated respones\n",
    "ai_response=model.invoke(\"Who is PM of Pakistan as well as Punjab?\")\n",
    "print(ai_response)\n",
    "\n",
    "# now we get the use of parser\n",
    "parser.invoke(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd79fe4",
   "metadata": {},
   "source": [
    "# LCEL Chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create our chain\n",
    "chain_prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"Respond to the user query\n",
    "    Query = {query}\n",
    "    \"\"\"\n",
    ")\n",
    "invoked=chain_prompt.invoke({\"query\":\"enlist 10 popular tv shows served on PTV HOME\"})\n",
    "\n",
    "chain = chain_prompt | model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d210881",
   "metadata": {},
   "source": [
    "## Creating custom tools\n",
    "LLM are smarter they know better which tool to choose the can decide wisely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b758326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "#must add doc string\n",
    "def multiply_tool(first_num:int,second_num:int):\n",
    "    \"\"\"This function multiply or find product out of two numbers\"\"\"\n",
    "    return first_num*second_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f8672",
   "metadata": {},
   "source": [
    "### Binding tools with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a70d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[multiply_tool]\n",
    "model_with_tools=model.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2=model_with_tools.invoke(\"Multiply 2 with 3\")\n",
    "print(response2.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47112ae",
   "metadata": {},
   "source": [
    "# PreBuilt TooLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52fc8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "search_tool=TavilySearchResults(max_results=5)\n",
    "tools = [search_tool]\n",
    "result=search_tool.invoke(\"What's the current rate of $ in Pakistan ?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a95f5",
   "metadata": {},
   "source": [
    "## - Creating our first agent with toooooooolsss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae96268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langgraph.prebuilt import create_react_agent ---> It is depricated in latest version of langchain + langgraph\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "my_agent=create_agent(\n",
    "    model=model,\n",
    "    tools=tools\n",
    ")\n",
    "print(\"Agent is created successfully...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"who is founder of Apple\"\n",
    "agent_response=my_agent.invoke({\"messages\":[(\"user\",query)]})\n",
    "agent_response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c83a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"search for best hotels near me\"\n",
    "agent_response=my_agent.invoke({\"messages\":[(\"user\",query)]})\n",
    "agent_response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb8003",
   "metadata": {},
   "source": [
    "## Creating the state schema -----pass only the data that is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ea4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use pydantic basemodel to get structured output\n",
    "\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "\"\"\"Suppose you wants to upload a post onto facebook\n",
    "   what all things you require a title\n",
    "   a photo (optional) \n",
    "   --photo can't be a GIF\n",
    "   \"\"\"\n",
    "\n",
    "class FacebookPost(BaseModel):\n",
    "    post_title:str=Field(description=\"Your Post title\")\n",
    "    post_tags:list[str]=Field(description=\"Catchy___Tags\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89eca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c47076",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure=model.with_structured_output(FacebookPost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_of_strucured_output=structure.invoke(\"create a facebook post related to discovery of jungle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a74270",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TITLE------>\",result_of_strucured_output.post_title)\n",
    "print(\"TAGS------->\",result_of_strucured_output.post_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471b406",
   "metadata": {},
   "source": [
    "#### _____________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f366f4",
   "metadata": {},
   "source": [
    "## Adding memory to agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.agents import create_agent\n",
    "import time\n",
    "print(\"______________Creating memory slot in your space________________\")\n",
    "time.sleep(2)\n",
    "print(\"Memory is beeeeeN   Added to your agent\")\n",
    "#-create memory object\n",
    "\n",
    "memory=MemorySaver()\n",
    "\n",
    "#-Binding memory to the agent\n",
    "\n",
    "agent2=create_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    checkpointer=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39d018",
   "metadata": {},
   "source": [
    "## __________________________________________________\n",
    "#### Managing Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374790b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-one single chat has 1 session and a session id which has memory thorughout the chat\n",
    "config1 ={\"configurable\": {\n",
    "    \"thread_id\":\"chat_1\"\n",
    "}}\n",
    "\n",
    "agent2.invoke({\"messages\":\"My name is Zain\"} , config=config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=agent2.invoke({\"messages\" : \"What is my name\"},config=config1)\n",
    "r[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc12400",
   "metadata": {},
   "source": [
    "# _________________________________________________________\n",
    "### Text to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753243b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Data ingestion\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(file_path=\"text_file.txt\")\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f226dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting text\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter()\n",
    "final_doc=splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0703a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-converting into embeddings\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedding_model=OllamaEmbeddings(model=\"all-minilm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533520d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-creating a vector store FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models.\n",
    "vector_store=FAISS.from_documents(\n",
    "    documents=final_doc, #----docs are our original data source\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "retriever=vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"Engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a830137",
   "metadata": {},
   "source": [
    "## _________________________________________________________________________________________\n",
    "### Creating RAG Chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19254cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Unable to import \"create_retrieval_chain\" thats why this session is Closed\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ad91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There is also a concept of HITL (Human in the loop) we will cover that in langraph module.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d73e2",
   "metadata": {},
   "source": [
    "### Tracking of our projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c799a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd08c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this in your environment varaibles\n",
    "LANGCHAIN_TRACING_V2=True\n",
    "LANGCHAIN_API_KEY=\"your_api_key\"\n",
    "LANGCHAIN_PROJECT=\"Project-name\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
